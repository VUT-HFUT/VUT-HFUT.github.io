<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Visual Understanding</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Visual Understanding</h1>
</div>
<p>This part covers the researches related to visual understanding, which focuses on Crowd Counting, Visual Grounding, Video Grounding, and Temporal Action Localization (TAL). Crowd Counting is a task to count people in image. Different from object detection, Crowd Counting aims at recognizing arbitrarily sized targets in various situations including sparse and cluttering scenes at the same time. Visual Grounding aims to localize a visual region in the given image referred by a natural language query. Video Grounding aims locate the queried action or event in an untrimmed video based on rich linguistic descriptions.
</p>
<h2>Crowd Counting</h2>
<ul>
<li><p><a href="https:////dl.acm.org/citation.cfm?doid=3343031.3350881" target=&ldquo;blank&rdquo;>DADNet: Dilated-Attention-Deformable ConvNet for Crowd Counting</a> (<i><b>Oral presentation</b></i>) <br />
Dan Guo, Kun Li*, Zheng-Jun Zha, Meng Wang. <br />
ACM International Conference on Multimedia (<b>ACM MM</b>), 2019. <br />

</p>
</li>
</ul>
<h2>Temporal Action Detection</h2>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9259546" target=&ldquo;blank&rdquo;>AOPNet: Anchor Offset Prediction Network for Temporal Action Proposal Generation</a> <br /> Fan Peng, Kun Li*, Xueliang Liu, and Dan Guo. <br />
International Conference on Signal Processing, Communications and Computing (ICSPCC), 2020.

</p>
</li>
</ul>
<h2>Video Grounding</h2>
<ul>
<li><p><a href="http://scis.scichina.com/en/2023/202102.pdf" target=&ldquo;blank&rdquo;>ViGT: proposal-free video grounding with a learnable token in the transformer</a> <br />
Kun Li, Dan Guo*, Meng Wang*. <br />
SCIENCES CHINA Information Sciences (<b>SCIS</b>), 2023. <br />
</p>
</li>
</ul>
<ul>
<li><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16285" target=&ldquo;blank&rdquo;>Proposal-Free Video Grounding with Contextual Pyramid Network</a> <br />
Kun Li, Dan Guo*, Meng Wang*. <br />
AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021. <br />
</p>
</li>
</ul>
<ul>
<li><p><a href="http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?flag=2&amp;file_no=202202090000002&amp;journal_id=jig" target=&ldquo;blank&rdquo;>Proposal-free Video Grounding based on Motion Excitation</a> <br /> 
Yichen Guo, Kun Li, Dan Guo*. <br />
Journal of Image and Graphics, 2023. <br />
</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s11280-022-01105-3" target=&ldquo;blank&rdquo;>Spatiotemporal Contrastive Modeling for Video Moment Retrieval</a> <br />
Yi Wang, Kun Li*, Guoliang Chen*, Yan Zhang, Dan Guo, Meng Wang. <br />
World Wide Web, 2023. <br /> 
</p>
</li>
</ul>
<h2>Visual Grounding</h2>
<ul>
<li><p><a href="https://dl.acm.org/doi/10.1145/3587251" target=&ldquo;blank&rdquo;>Transformer-based Visual Grounding with Cross-modality Interaction</a> <br /> 
Kun Li, Jiaxiu Li, Dan Guo*, Xun Yang*, Meng Wang*. <br /> 
ACM Transactions on Multimedia Computing, Communications, and Applications (<b>TOMM</b>), 2023 <br /> 
</p>
</li>
</ul>
<h2>Human Beahavior Analysis</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2307.10624" target=&ldquo;blank&rdquo;>Joint Skeletal and Semantic Embedding Loss for Micro-gesture Classification</a> <br />
Kun Li, Dan Guo*, Guoliang Chen, Xinge Peng, Meng Wang*. <br />
MiGA@IJCAI23: International IJCAI Workshop on Micro-gesture Analysis for Hidden Emotion Understanding, 2023. <br />
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2308.01526" target=&ldquo;blank&rdquo;>Data Augmentation for Human Behavior Analysis in Multi-Person Conversations</a> <br />
Kun Li, Dan Guo*, Guoliang Chen, Feiyang Liu, Meng Wang*. <br />
ACM International Conference on Multimedia (<b>ACM MM</b>), 2023. <br />
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-10-29 11:27:16 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</div>
</body>
</html>
